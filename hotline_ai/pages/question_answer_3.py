import streamlit as st
from st_pages import Page, Section, show_pages, add_page_title
from st_pages import show_pages_from_config

add_page_title()

st.markdown("## LLM with Retrieval Argument (RAG)")
st.markdown( 
    """
    * The idea of integrating large language model (LLM) like LLMA with Retrieval-Augmented Generation method
    is to enhance both the quality and relevance of generated content with more natural human response like instead
    of a generic return of text data.
    * RAG combines the strengths of retrieval-based on generation based appraoches to improve the overall performance of
    language models in various tasks.
    * Here we Combine the outputs generated by RAG with LLM. This combination can result in content that is both contextually accuracte
    based on `input data` and creatively coherent (LLM-generated)

    """
)

st.markdown( 
    """
    ## System Architecture Wiki 3.0 | Decoder and Encoder 
    * Here we implement the LLM and retrieveal system based on Llam2 model and RAG
    * The retrieval system is still similar to `wiki 2.0`. In this `wiki 3.0`, we add an Artificial Intelligent
    element based on NLP model. 
    * The LLM model will accept the user `prompt` and top `ranked` result from `knowledge base` and generate the answer
    based on Natural Human Response.
    * For this whole system to connect, we're using framework so called `LangChain ü¶úÔ∏è`.
    * LangChain is a framework that designed to facilitate the development of applications powered by LLM.
    """
)

st.image("./images/wiki_3_0_decoder_encoder.jpg", caption="Fig 2: Hardware Requirements")


st.markdown("""
    ## Suggested Prompt without Memory
    * I've to experiment with different prompt to ensure the model provide more precise and direce answer and preventing
      it from generating more text which is not related to actual answer.
    * This prompt at least to my resarch and study help to control the generation of this text.
    * This prompt didn't contain `Memory` or `chat_history` therefore it won't remember past event. 
            
            """)
st.code("""[INST] <<SYS>>
Your name is Kelly, you are a helpful, respectful and honest assistant. Always answer as helpfully as possible using the context text provided.
You answer should only answer the question once and not have any text after the answer is done.\n\nIf a question does not make any sense, or is not factually
coherent, explain why instead of answering something not correct. If you don't know the answer, just say you don't know and submit the request to hotline@xfab.com for further assistance.\n
<</SYS>>
CONTEXT:/n/n {context}/n
Question: {question}
[/INST]

"""        
)

st.markdown("""
    ## Suggested Prompt with Memory
    * I've to experiment with different prompt to ensure the model provide more precise and direce answer and preventing
      it from generating more text which is not related to actual answer.
    * This prompt at least to my resarch and study help to control the generation of this text.
    * This prompt contain `Memory` or `chat_history` that allow it to remember past event.
            
    """)

st.code("""[INST] <<SYS>>
Your name is Kelly, you are a helpful, respectful and honest assistant. Always answer as helpfully as possible using the context text provided.
You answer should only answer the question once and not have any text after the answer is done.\n\nIf a question does not make any sense, or is not factually
coherent, explain why instead of answering something not correct. If you don't know the answer, just say you don't know and submit the request to hotline@xfab.com for further assistance.\n
<</SYS>>
CONTEXT:/n/n {context}/n
{chat_history}
Question: {question}
[/INST]

""")


st.markdown("""
            ## Retrieval Argumented-Generation Method
            
            ### Conversational Question Answer Retrieval Chain
            """
            )

st.code("""
    def conversationalretrieval_qa_chain(llm, prompt, db, memory):
        chain_type_kwargs = {"prompt": prompt}
        qa_chain = ConversationalRetrievalChain.from_llm(llm=llm,
                                                        chain_type= 'stuff',
                                                        retriever=db.as_retriever(search_kwargs={'k': 1}),
                                                        verbose=True,
                                                        memory=memory,
                                                        combine_docs_chain_kwargs=chain_type_kwargs
                                                        )
        return qa_chain
        
        
        """)

st.markdown("""### Load Question Answer Chain""")
st.code("""
    def init_chain(llm,prompt,memory):
        chain=load_qa_chain(llm, chain_type="stuff",memory=memory, prompt=prompt)
        return chain
        """)

st.markdown("""### Retriever Question Answer Chain""")
st.code("""
    def retrieval_qa_chain(llm, prompt, db, memory):
        chain_type_kwargs = {"prompt": prompt, "memory": memory}
        qa_chain = RetrievalQA.from_chain_type(llm=llm,
                                        chain_type='stuff',
                                        retriever=db.as_retriever(), #search_kwargs={'k': 1}
                                        return_source_documents=True,
                                        chain_type_kwargs=chain_type_kwargs
                                        )
        return qa_chain
        """)


st.markdown("""
    ## Llama 2 Wrapper
    * These are collections of Llama 2 wrapper for different sources.
    * Provided wrapper support `quantized` model `4bit` precision that enable it to run on lower spec machine.
    * These wrapper based on my research support GPU and CPU for inference.
    """)

st.markdown("""### CTransformers""")

st.code("""

# GPU SUpported
llm = CTransformers(model=MODEL_PATH,
                    model_type='llama',
                    config={'max_new_tokens': 2048,
                            'temperature': 0.7,
                            'gpu_layers': 35,
                            'stream' : True,
                            },
                    
                   )
LLM_Chain=LLMChain(prompt=prompt, llm=llm,memory=memory, verbose=True)

# CPU supported
llm = CTransformers(model=MODEL_PATH,
                    model_type='llama',
                    config={'max_new_tokens': 128,
                            'temperature': 0.01}
                   )
LLM_Chain=LLMChain(prompt=prompt, llm=llm)

""")

st.markdown("""### CTransformers Hugging Face""")
st.code("""

from ctransformers import AutoModelForCausalLM
model_path = r"D:/llama2_quantized_models/7B_q5/llama-2-7b-chat.ggmlv3.q5_K_M.bin"

model_name = "llama-2-7b-chat.ggmlv3.q5_K_M.bin"

#Run with CPU
#llm = AutoModelForCausalLM.from_pretrained('TheBloke/Llama-2-7B-Chat-GGML', model_file=model)

# Run with GPU
#llm = AutoModelForCausalLM.from_pretrained('TheBloke/Llama-2-7B-Chat-GGML', model_file=model_path, gpu_layers=35)

# Run and load model locally with GPU
llm = AutoModelForCausalLM.from_pretrained(model_path_or_repo_id=model_path, model_file=model_name, gpu_layers=35, model_type='llama')

for word in llm("Write me a python code that that scrap the webpage?", stream=True):
    print(word, end='')

""")



st.markdown("""### LlamaCpp""")

st.code("""

# GPU and CPU supported
callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])

llm = LlamaCpp(
    model_path= MODEL_PATH,
    max_tokens=256,
    n_gpu_layers=32,
    n_batch= 512, #256,
    callback_manager=callback_manager,
    n_ctx= 1024,
    verbose=True,
    temperature=0.8,
)
print(llm.n_gpu_layers)

LLM_Chain=LLMChain(prompt=prompt, memory=memory, llm=llm)

""")



col1, col2= st.columns(2)

with col1:
   st.header("Pros")
   st.markdown(""" 
               ### Enhanced Relevance
               * The retriever ensures that the generated content is grounded in relevant information from the corpus, making the generated responses more accurate and contextually appropriate.
               
               ### Creative Generation
               * The LLM contributes to generating creative and fluent responses, even if the retrieved passages themselves might be concise or technical.
               
               ### Provide Constant and Similar Response
               * The retriever narrows down the context, helping the LLM to produce content that is less likely to be ambiguous or misinterpreted.
               
               """)
   

with col2:
   st.header("Cons")
   st.markdown("""
               ### Hardware Requirements
               * Require decent GPU to run the model and perform the inference. 
               
               ### Retriever and Data Set Quality
               * The quality of the retrieverselection and clean data set is crucial. If irrelevant or incorrect passages are retrieved, the generated content might also be misguided.
               
               ### Fine-Tuning Balance
               * Striking the right balance between the LLM's creative generation and the retriever's factual context is essential for generating accurate yet engaging content.
               Therefore selection of `prompt` and `temperature` control is very important.
               
               """)
st.markdown("""## Key Takeaways
* In practice, the integration of LLM with the RAG method requires experimentation, tuning, and optimization to achieve the desired performance for your specific application.""")
