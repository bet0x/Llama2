{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qoTpJcI0OadN"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu121\n",
        "!pip install autotrain-advanced\n",
        "#!pip install ipython\n",
        "!pip install huggingface-hub"
      ],
      "metadata": {
        "id": "as6ckB4yOkNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!autotrain setup --update-torch"
      ],
      "metadata": {
        "id": "GT2gMnbOsdSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(\"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\")"
      ],
      "metadata": {
        "id": "RSm80FDfOliz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!autotrain llm --train --project_name rewiki --model meta-llama/Llama-2-7b-hf --data_path . --use_peft --use_int4 --learning_rate 2e-4 --train_batch_size 4 --num_train_epochs 12 --trainer sft --push_to_hub --repo_id Captluke/Llama-2-7b-finetune-v2 > training.log &"
      ],
      "metadata": {
        "id": "A66oo78KOvjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /root/.cache/huggingface/hub/models--abhishek--llama-2-7b-hf-small-shards/snapshots/c9dfa5fcc6ba6501955c19286af42ba80d74228d/"
      ],
      "metadata": {
        "id": "Se-nzWv9QEV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "-5X9OOTto8_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global credential.helper store"
      ],
      "metadata": {
        "id": "GrC4Zg2a3V7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install autotrain-advanced\n",
        "!pip install huggingface-hub\n",
        "!pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu121\n"
      ],
      "metadata": {
        "id": "SUHA5xj93t3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers einops accelerate langchain bitsandbytes\n",
        "!pip install xformers"
      ],
      "metadata": {
        "id": "eNYguVXIYNLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from torch.nn import DataParallel\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "import transformers\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n"
      ],
      "metadata": {
        "id": "BU4ZKhzgJTKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenizer = AutoTokenizer.from_pretrained(\"Captluke/Llama-2-7b-finetune-v1\")\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"Captluke/Llama-2-7b-finetune-v1\")"
      ],
      "metadata": {
        "id": "NX2tEuHCJis6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import transformers\n",
        "import torch\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "model = \"Captluke/Llama-2-7b-finetune-v1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Captluke/Llama-2-7b-finetune-v1\")\n",
        "pipeline=transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        "    max_length=1000,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "llm=HuggingFacePipeline(pipeline=pipeline, model_kwargs={'temperature':0})\n",
        "\n",
        "prompt = '''what is Flatpv ?'''\n",
        "\n",
        "print(llm(prompt))"
      ],
      "metadata": {
        "id": "ZwV0fjNvJkDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hpvbc8z0ZiR9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}