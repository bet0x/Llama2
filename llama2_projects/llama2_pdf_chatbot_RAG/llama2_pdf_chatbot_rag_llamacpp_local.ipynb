{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "Method to use `llama_cpp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import transformer classes for generaiton\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "# Import torch for datatype attributes\n",
    "import torch\n",
    "from llama_cpp import Llama\n",
    "\n",
    "from langchain.llms import LlamaCpp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = r\"D:/llama2_quantized_models/7B_q5/llama-2-7b-chat.ggmlv3.q5_K_M.bin\"\n",
    "model_name = \"llama-2-7b-chat.ggmlv3.q5_K_M.bin\"\n",
    "\n",
    "llm = Llama(model_path=model_path,\n",
    "            n_gpu_layers=32, \n",
    "            n_ctx=8192, \n",
    "            n_batch=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"write me a resignation letter\"\n",
    "output = llm(prompt, \n",
    "                 max_tokens=-1, \n",
    "                 echo=False, \n",
    "                 temperature=0.2, \n",
    "                 top_p=0.1)\n",
    "\n",
    "print(output['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method to use RAG\n",
    "This method we're using `RAG` with `llama_cpp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain import PromptTemplate\n",
    "from langchain import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = r\"D:/llama2_quantized_models/7B_chat/llama-2-7b-chat.ggmlv3.q4_K_M.bin\"\n",
    "\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "llm = LlamaCpp(\n",
    "    model_path= MODEL_PATH,\n",
    "    max_tokens=-1,\n",
    "    n_gpu_layers=32,\n",
    "    n_batch= 512, #256,\n",
    "    callback_manager=callback_manager,\n",
    "    n_ctx= 8192, #2048, #1024,\n",
    "    verbose=False,\n",
    "    temperature=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import transformer classes for generaiton\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "# Import torch for datatype attributes\n",
    "import torch\n",
    "\n",
    "import os\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain import PromptTemplate,LLMChain\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from llama_index import LLMPredictor, PromptHelper, GPTVectorStoreIndex\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "# Bring in stuff to change service context\n",
    "from llama_index import set_global_service_context\n",
    "from llama_index import ServiceContext\n",
    "\n",
    "# Bring in embeddings wrapper\n",
    "from llama_index.embeddings import LangchainEmbedding\n",
    "\n",
    "# Bring in HF embeddings - need these to represent document chunks\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Import deps to load documents\n",
    "from llama_index import VectorStoreIndex, download_loader\n",
    "from pathlib import Path\n",
    "\n",
    "# Bring in stuff to change service context\n",
    "from llama_index import set_global_service_context\n",
    "from llama_index import ServiceContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1\n",
    "llm_predictor = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and dl embeddings instance\n",
    "embeddings=LangchainEmbedding(\n",
    "    HuggingFaceEmbeddings(model_name=\"all-mpnet-base-v2\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new service context instance\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    chunk_size=500,\n",
    "    llm=llm,\n",
    "    embed_model=embeddings\n",
    ")\n",
    "# And set the service context\n",
    "set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download PDF Loader\n",
    "PyMuPDFReader = download_loader(\"PyMuPDFReader\")\n",
    "\n",
    "# Create PDF Loader\n",
    "loader = PyMuPDFReader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents\n",
    "file = r\"C:/Users/Lukas/Desktop/My_Projects/To_Upload/Llama2/llama2_projects/llama2_pdf_chatbot_faiss_windows/data/V3/Hotline_Wiki_v3.pdf\"\n",
    "documents = loader.load(file_path=Path(file), metadata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an index - we'll be able to query this in a sec\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup index query engine using LLM\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out a query in natural\n",
    "response = query_engine.query(\"how to setup alpha pdk using json file ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLAMA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
