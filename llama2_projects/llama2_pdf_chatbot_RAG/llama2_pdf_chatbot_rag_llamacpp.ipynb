{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "Method to use `llama_cpp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import transformer classes for generaiton\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "# Import torch for datatype attributes\n",
    "import torch\n",
    "from llama_cpp import Llama\n",
    "\n",
    "from langchain.llms import LlamaCpp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "model_path = r\"D:/llama2_quantized_models/7B_q5/llama-2-7b-chat.ggmlv3.q5_K_M.bin\"\n",
    "model_name = \"llama-2-7b-chat.ggmlv3.q5_K_M.bin\"\n",
    "\n",
    "llm = Llama(model_path=model_path,\n",
    "            n_gpu_layers=32, \n",
    "            n_ctx=8192, \n",
    "            n_batch=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " due to bullying at work\n",
      "\n",
      "I am writing to inform you of my decision to resign from my position as [position] at [company name], effective [date of last day of work]. The reason for my resignation is the persistent and unacceptable bullying behavior that I have endured during my time here.\n",
      "Despite my best efforts to address this issue through the proper channels, including speaking with HR and my supervisor, the behavior has continued and has created a hostile work environment that I can no longer tolerate. I have tried to focus on my work and contribute to the team, but the constant harassment and intimidation have made it impossible for me to do so.\n",
      "I am deeply saddened to leave [company name] under these circumstances, as I have truly enjoyed my time here and have valued the opportunities that I have had to grow and develop professionally. However, I cannot allow my well-being and mental health to be compromised any longer.\n",
      "I want to make it clear that I do not hold any personal grievances against my colleagues or supervisors, but rather I am resigning due to the systemic issues within the company that have allowed this behavior to persist. I hope that in the future, [company name] will take steps to address these issues and create a workplace where all employees can feel safe and respected.\n",
      "I will do everything possible to ensure a smooth transition of my responsibilities and complete any outstanding tasks before my departure. If there is anything specific that you would like me to focus on during my remaining time here, please let me know.\n",
      "Thank you for your understanding, and I wish the company all the best in its future endeavors.\n",
      "Sincerely,\n",
      "[Your Name]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"write me a resignation letter\"\n",
    "output = llm(prompt, \n",
    "                 max_tokens=-1, \n",
    "                 echo=False, \n",
    "                 temperature=0.2, \n",
    "                 top_p=0.1)\n",
    "\n",
    "print(output['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method to use RAG\n",
    "This method we're using `RAG` with `llama_cpp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain import PromptTemplate\n",
    "from langchain import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = r\"D:/llama2_quantized_models/7B_chat/llama-2-7b-chat.ggmlv3.q4_K_M.bin\"\n",
    "\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "llm = LlamaCpp(\n",
    "    model_path= MODEL_PATH,\n",
    "    max_tokens=-1,\n",
    "    n_gpu_layers=32,\n",
    "    n_batch= 512, #256,\n",
    "    callback_manager=callback_manager,\n",
    "    n_ctx= 8192, #2048, #1024,\n",
    "    verbose=False,\n",
    "    temperature=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "binary_path: c:\\Users\\Lukas\\anaconda3\\envs\\LLAMA\\Lib\\site-packages\\bitsandbytes\\cuda_setup\\libbitsandbytes_cuda116.dll\n",
      "CUDA SETUP: Loading binary c:\\Users\\Lukas\\anaconda3\\envs\\LLAMA\\Lib\\site-packages\\bitsandbytes\\cuda_setup\\libbitsandbytes_cuda116.dll...\n"
     ]
    }
   ],
   "source": [
    "# Import transformer classes for generaiton\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "# Import torch for datatype attributes\n",
    "import torch\n",
    "\n",
    "import os\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain import PromptTemplate,LLMChain\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from llama_index import LLMPredictor, PromptHelper, GPTVectorStoreIndex\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "# Bring in stuff to change service context\n",
    "from llama_index import set_global_service_context\n",
    "from llama_index import ServiceContext\n",
    "\n",
    "# Bring in embeddings wrapper\n",
    "from llama_index.embeddings import LangchainEmbedding\n",
    "\n",
    "# Bring in HF embeddings - need these to represent document chunks\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Import deps to load documents\n",
    "from llama_index import VectorStoreIndex, download_loader\n",
    "from pathlib import Path\n",
    "\n",
    "# Bring in stuff to change service context\n",
    "from llama_index import set_global_service_context\n",
    "from llama_index import ServiceContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1\n",
    "llm_predictor = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and dl embeddings instance\n",
    "embeddings=LangchainEmbedding(\n",
    "    HuggingFaceEmbeddings(model_name=\"all-mpnet-base-v2\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new service context instance\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    chunk_size=500,\n",
    "    llm=llm,\n",
    "    embed_model=embeddings\n",
    ")\n",
    "# And set the service context\n",
    "set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download PDF Loader\n",
    "PyMuPDFReader = download_loader(\"PyMuPDFReader\")\n",
    "\n",
    "# Create PDF Loader\n",
    "loader = PyMuPDFReader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents\n",
    "file = r\"C:/Users/Lukas/Desktop/My_Projects/To_Upload/Llama2/llama2_projects/llama2_pdf_chatbot_faiss_windows/data/V3/Hotline_Wiki_v3.pdf\"\n",
    "documents = loader.load(file_path=Path(file), metadata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an index - we'll be able to query this in a sec\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup index query engine using LLM\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To set up an alpha PDK using a JSON file, you can follow these steps:\n",
      "1. Generate a JSON file: After logging into your X-FAB portal, navigate to Design > SpecXplorer. Select your target process and click either \"Select modules\" or \"Select modules by devices.\" Choose the desired modules from your PDK setup.\n",
      "2. Customize PDK settings: Once you have selected the desired modules, you can customize your PDK settings using a JSON file. The JSON file should contain details such as naming conventions, design rules, and other parameters that define how your PDK should be set up. You can generate this file using SpecXplorer or create it manually.\n",
      "3. Upload the JSON file: After generating or customizing the JSON file, you need to upload it to the alpha PDK setup. To do this, navigate to Design > Alpha PDK in your X-FAB portal and select \"Upload\" next to the \"JSON File\" field. Then, select the JSON file you created earlier and click \"Open.\"\n",
      "4. Configure the PDK: Once you have uploaded the JSON file, you can configure your alpha PDK settings according to the parameters defined in the file. You will be able to see the configuration options for each parameter on the Alpha PDK setup page. Select the desired values for each parameter and click \"Apply\" to save the changes.\n",
      "5. Verify the PDK: After configuring your alpha PDK, it's essential to verify that everything is set up correctly. You can do this by running a DRC check using the alpha PDK setup or by simulating the design flow using SpecXplorer. This will help you identify any potential issues and ensure that your design flows smoothly through the manufacturing process."
     ]
    }
   ],
   "source": [
    "# Test out a query in natural\n",
    "response = query_engine.query(\"how to setup alpha pdk using json file ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLAMA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
