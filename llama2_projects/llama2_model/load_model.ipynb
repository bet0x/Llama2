{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Ctransformers\n",
    "In general `ggml` and `gptq` are not supported by HuggingFace Transformers. However, this work using Ctransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctransformers import AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = r\"D:/llama2_quantized_models/7B_q5/llama-2-7b-chat.ggmlv3.q5_K_M.bin\"\n",
    "model_name = \"llama-2-7b-chat.ggmlv3.q5_K_M.bin\"\n",
    "\n",
    "# check ctransformers doc for more configs\n",
    "config = {'max_new_tokens': 256, 'repetition_penalty': 1.1, \n",
    "          'temperature': 0.1, 'stream': True}\n",
    "\n",
    "llm = AutoModelForCausalLM.from_pretrained(model_path_or_repo_id=model_path, model_file=model_name, gpu_layers=35, model_type='llama', **config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"Write a poem to help me remember the first 10 elements on the periodic table, giving each\n",
    "element its own line.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 6113, 263, 26576, 304, 1371, 592, 6456, 278, 937, 29871, 29896, 29900, 3161, 373, 278, 29591, 1591, 29892, 6820, 1269, 13, 5029, 967, 1914, 1196, 29889]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the prompt\n",
    "tokens = llm.tokenize(prompt)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline Execution\n",
    "llm(prompt, stream=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream execution for Generation + Stats\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "NUM_TOKENS=0\n",
    "print('-'*4+'Start Generation'+'-'*4)\n",
    "for token in llm.generate(tokens):\n",
    "    print(llm.detokenize(token), end='', flush=True)\n",
    "    NUM_TOKENS+=1\n",
    "time_generate = time.time() - start\n",
    "print('\\n')\n",
    "print('-'*4+'End Generation'+'-'*4)\n",
    "print(f'Num of generated tokens: {NUM_TOKENS}')\n",
    "print(f'Time for complete generation: {time_generate}s')\n",
    "print(f'Tokens per secound: {NUM_TOKENS/time_generate}')\n",
    "print(f'Time per token: {(time_generate/NUM_TOKENS)*1000}ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Llama_cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "model_path = r\"D:/llama2_quantized_models/7B_q5/llama-2-7b-chat.ggmlv3.q5_K_M.bin\"\n",
    "model_name = \"llama-2-7b-chat.ggmlv3.q5_K_M.bin\"\n",
    "\n",
    "llm = Llama(model_path=model_path,\n",
    "            n_gpu_layers=32, \n",
    "            n_ctx=8192, \n",
    "            n_batch=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "    [INST] <<SYS>>\n",
    "    You are a helpful, respectful and honest assistant. \n",
    "    Always answer as helpfully as possible, while being safe.  \n",
    "    Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. \n",
    "    Please ensure that your responses are socially unbiased and positive in nature.\n",
    "    If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. \n",
    "    If you don't know the answer to a question, please don't share false information.\n",
    "    <</SYS>>\n",
    "    {INSERT_PROMPT_HERE} [/INST]\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create prompt\n",
    "prompt = 'Write me a resignation letter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm(prompt, \n",
    "                 max_tokens=-1, \n",
    "                 echo=False, \n",
    "                 temperature=0.2, \n",
    "                 top_p=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " due to bullying at work\n",
      "\n",
      "I am writing to inform you of my decision to resign from my position as [position] at [company name], effective [date of last day of work]. The reason for my resignation is the persistent and unacceptable behavior of [name of bully or supervisor], which has created a hostile work environment for me.\n",
      "Despite my efforts to address this issue through the company's reporting mechanisms, including speaking with [name of HR representative or supervisor], I have not seen any meaningful action taken to address the problem. As a result, I feel that it is necessary to take this step in order to protect my own well-being and maintain my dignity as an employee.\n",
      "I want to make it clear that I have enjoyed my time at [company name] and have appreciated the opportunities for growth and development that I have had here. However, I cannot continue to work in an environment where I am subjected to bullying and harassment on a regular basis.\n",
      "I will do everything possible to ensure a smooth transition of my responsibilities and complete any outstanding tasks before my departure. If there is anything specific that you would like me to focus on during my remaining time here, please let me know.\n",
      "Thank you for your understanding, and I wish the company continued success in the future.\n",
      "Sincerely,\n",
      "[Your name]\n"
     ]
    }
   ],
   "source": [
    "print(output['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLAMA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
