{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "277fdb47",
   "metadata": {},
   "source": [
    "# llama2 PDF Chatbot with Pinecone and llama.cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6264326d",
   "metadata": {},
   "source": [
    "## Step 1: Install and import all the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f92c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install farm-haystack\n",
    "!pip install accelerate\n",
    "!pip install sentence_transformers\n",
    "!pip install streamlit chainlit langchain openai wikipedia chromadb tiktoken\n",
    "!pip install pypdf\n",
    "!pip install ctransformers\n",
    "!pip install streamlit-chat\n",
    "!pip install bitsandbytes-cuda112\n",
    "!pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu121\n",
    "!pip install pinecone-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdf8e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, OnlinePDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "import pinecone\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3abc49",
   "metadata": {},
   "source": [
    "## Step 2: Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a7dc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:/Users/Lukas/Desktop/My_Projects/To_Upload/Llama2/llama2_projects/llama2_pdf_chatbot_faiss_windows/data/Hotline_Wiki.pdf\"\n",
    "\n",
    "#loader = OnlinePDFLoader(\"https://wolfpaulus.com/wp-content/uploads/2017/05/field-guide-to-data-science.pdf\")\n",
    "loader = PyPDFLoader(path)\n",
    "#loader = PyPDFLoader(\"/content/The-Field-Guide-to-Data-Science.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5b20ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f816efa5",
   "metadata": {},
   "source": [
    "## Step 3: Split the Text into Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0df3753",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70188b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47422867",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca2a25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f860c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8921f231",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e346a45",
   "metadata": {},
   "source": [
    "## Step 4: Setup Pinecone Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01646c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY', 'aa5d1b66-d1d9-451a-9f6b-dfa32db988fc')\n",
    "PINECONE_API_ENV = os.environ.get('PINECONE_API_ENV', 'us-west1-gcp-free')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd8f87c",
   "metadata": {},
   "source": [
    "## Step 5: Download the Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe877d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "query_result=embeddings.embed_query(\"Hello\")\n",
    "len(query_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1538672d",
   "metadata": {},
   "source": [
    "## Step 6: Initializing the Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9184039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize pinecone which can be copied from Pinecone 'Connect' button\n",
    "pinecone.init( \n",
    "    api_key=PINECONE_API_KEY,  # find at app.pinecone.io\n",
    "    environment=PINECONE_API_ENV,  # next to api key in console\n",
    ")\n",
    "index_name = \"llama2-pdf-chatbox\" # put in the name of your pinecone index here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c85bf73",
   "metadata": {},
   "source": [
    "## Step 7: Create Embeddings for Each of the Text Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c327dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch=Pinecone.from_texts([t.page_content for t in docs], embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1d6f93",
   "metadata": {},
   "source": [
    "## Step 8: If you already have an index, you can load it like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f223bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = Pinecone.from_existing_index(index_name, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e37ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cc5102",
   "metadata": {},
   "source": [
    "## Step 9: Similarity Search (Semantic Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34f8074",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"how to start alpha pdk using xkit ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c7bec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=docsearch.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c62e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a5350e",
   "metadata": {},
   "source": [
    "## Step 10: Query the Docs to get the Answer Back using Llama 2 model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b5c05b",
   "metadata": {},
   "source": [
    "### Installation with OpenBLAS / cuBLAS / CLBlast / Metal\n",
    "\n",
    "`llama.cpp` supports multiple BLAS backends for faster processing. Use the `FORCE_CMAKE=1` environment variable to force the use of `cmake` and install the pip package for the desired BLAS backend.\n",
    "\n",
    "To install with `OpenBLAS`, set the LLAMA_BLAS and LLAMA_BLAS_VENDOR environment variables before installing:\n",
    "\n",
    "```\n",
    "-> !CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir --verbose\n",
    "\n",
    "-> CMAKE_ARGS=\"-DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS\" FORCE_CMAKE=1 pip install llama-cpp-python\n",
    "```\n",
    "\n",
    "To install with `cuBLAS (CUDA Support)`, set the `LLAMA_CUBLAS=1` environment variable before installing:\n",
    "\n",
    "```\n",
    "-> CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python\n",
    "```\n",
    "\n",
    "To install with `CLBlast`, set the `LLAMA_CLBLAST=1` environment variable before installing:\n",
    "\n",
    "```\n",
    "-> CMAKE_ARGS=\"-DLLAMA_CLBLAST=on\" FORCE_CMAKE=1 pip install llama-cpp-python\n",
    "```\n",
    "\n",
    "To install with Metal (MPS), set the `LLAMA_METAL=on` environment variable before installing:\n",
    "\n",
    "```\n",
    "CMAKE_ARGS=\"-DLLAMA_METAL=on\" FORCE_CMAKE=1 pip install llama-cpp-python\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884444af",
   "metadata": {},
   "source": [
    "### Setup Environment for Windows\n",
    "To set the variables `CMAKE_ARGS` and `FORCE_CMAKE` in PowerShell, follow the next steps (Example using, OpenBLAS):\n",
    "\n",
    "In this notebook, i'm using OpenBLAS as my backend\n",
    "\n",
    "I would suggest to refer to this guideline : https://github.com/abetlen/llama-cpp-python\n",
    "\n",
    "Conda\n",
    "```\n",
    "Without CUDA support\n",
    "To set an environment:\n",
    "(LLAMA) PS C:\\Users\\jlukas> $Env:CMAKE_ARGS=\"-DLLAMA_BLAS=on -DLLAMA_BLAS_VENDOR=openBLAS\"  \n",
    "(LLAMA) PS C:\\Users\\jlukas> $Env:FORCE_CMAKE=1    \n",
    "\n",
    "To check the environment\n",
    "(LLAMA) PS C:\\Users\\jlukas> Get-ChildItem Env:FORCE_CMAKE  \n",
    "(LLAMA) PS C:\\Users\\jlukas> Get-ChildItem Env:CMAKE_ARGS\n",
    "(LLAMA) PS C:\\Users\\jlukas> \"$Env:CMAKE_ARGS $Env:FORCE_CMAKE\"\n",
    "\n",
    "With CUDA support\n",
    "To set an environment:\n",
    "(LLAMA) PS C:\\Users\\jlukas> $Env:CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\"  \n",
    "(LLAMA) PS C:\\Users\\jlukas> $Env:FORCE_CMAKE=1    \n",
    "\n",
    "To check the environment\n",
    "(LLAMA) PS C:\\Users\\jlukas> Get-ChildItem Env:FORCE_CMAKE  \n",
    "(LLAMA) PS C:\\Users\\jlukas> Get-ChildItem Env:CMAKE_ARGS\n",
    "(LLAMA) PS C:\\Users\\jlukas> \"$Env:CMAKE_ARGS $Env:FORCE_CMAKE\"\n",
    "```\n",
    "\n",
    "Normal-Terminal\n",
    "```\n",
    "Without CUDA Support\n",
    "To set an environment:\n",
    "PS C:\\Users\\jlukas> $Env:CMAKE_ARGS=\"-DLLAMA_OPENBLAS=on -DLLAMA_BLAS_VENDOR=openBLAS\"\n",
    "PS C:\\Users\\jlukas> $Env:FORCE_CMAKE=1\n",
    "\n",
    "To check the environment\n",
    "PS C:\\Users\\jlukas> \"$Env:CMAKE_ARGS $Env:FORCE_CMAKE\"\n",
    "\n",
    "With CUDA support\n",
    "To set an environment:\n",
    "PS C:\\Users\\jlukas> $Env:CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\"\n",
    "PS C:\\Users\\jlukas> $Env:FORCE_CMAKE=1\n",
    "\n",
    "To check the environment\n",
    "PS C:\\Users\\jlukas> \"$Env:CMAKE_ARGS $Env:FORCE_CMAKE\"\n",
    "```\n",
    "\n",
    "Environment Variables\n",
    "\n",
    "```\n",
    "Alternatively, you can add this variable at Environment Variables as follow:\n",
    "\n",
    "Variable = CMAKE_ARGS\n",
    "Value = -DLLAMA_OPENBLAS=on -DLLAMA_BLAS_VENDOR=openBLAS\n",
    "\n",
    "or \n",
    "Value = -DLLAMA_CUBBLAS=on\n",
    "\n",
    "Variable = FORCE_CMAKE\n",
    "Value = 1\n",
    "```\n",
    "\n",
    "Restart your terminal and see if the changes take place.\n",
    "\n",
    "Then, call `pip` after setting the variables:\n",
    "```\n",
    "pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir\n",
    "```\n",
    "\n",
    "See the above instructions and set `CMAKE_ARGS` to the `BLAS backend` you want to use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266498ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from huggingface_hub import hf_hub_download\n",
    "from langchain.chains.question_answering import load_qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd8cb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "# Verbose is required to pass to the callback manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d7664e",
   "metadata": {},
   "source": [
    "## Quantized Models from the Hugging Face Community\n",
    "The Hugging Face community provides quantized models, which allow us to efficiently and effectively utilize the model on the T4 GPU. It is important to consult reliable sources before using any model.\n",
    "\n",
    "There are several variations available, but the ones that interest us are based on the GGLM library.\n",
    "\n",
    "We can see the different variations that Llama-2-13B-GGML has here.\n",
    "\n",
    "In this case, we will use the model called Llama-2-13B-chat-GGML.\n",
    "\n",
    "Quantization reduces precision to optimize resource usage.\n",
    "\n",
    "Quantization is a technique to reduce the computational and memory costs of running inference by representing the weights and activations with low-precision data types like 8-bit integer ( int8 ) instead of the usual 32-bit floating point ( float32 )."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d901ad24",
   "metadata": {},
   "source": [
    "## To download the quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa9fd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip this if you have this model downloaded already\n",
    "\n",
    "# model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGML\"\n",
    "# model_basename = \"llama-2-13b-chat.ggmlv3.q5_1.bin\" # the model is in bin format\n",
    "# model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e31547",
   "metadata": {},
   "source": [
    "## For CPU or GPU (if CuBLAS=ON) run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69935ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = r\"D:/AI_CTS/Llama2/llama2_projects/llama2_quantized_models/7B_chat/llama-2-7b-chat.ggmlv3.q8_0.bin\"\n",
    "#model_path = r\"C:/Users/Lukas\\Desktop/My_Projects/To_Upload/Llama2/llama2_projects/llama2_quantized_models/7B_chat/llama-2-7b-chat.ggmlv3.q8_0.bin\"\n",
    "\n",
    "n_gpu_layers = 40  # Change this value based on your model and your GPU VRAM pool.\n",
    "n_batch = 256  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "\n",
    "# Loading model,\n",
    "llm = LlamaCpp(\n",
    "    model_path=model_path,\n",
    "    max_tokens=256,\n",
    "    n_gpu_layers=40,\n",
    "    n_batch= 512, #256,\n",
    "    callback_manager=callback_manager,\n",
    "    n_ctx= 1024,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e53816a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain=load_qa_chain(llm,chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01a10b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"how to start alpha pdk using xkit ?\"\n",
    "docs=docsearch.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c80b6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6ed462",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7353933b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit to https://github.com/MuhammadMoinFaisal/LargeLanguageModelsProjects/blob/main/QA%20Book%20PDF%20LangChain%20Llama%202/Final_Llama_CPP_Ask_Question_from_book_PDF_Llama.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad7ac3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    query = input(f\"Prompt: \")\n",
    "    docs = docsearch.similarity_search(query)\n",
    "    if query == \"exit\":\n",
    "        print(\"Exiting\")\n",
    "    if query == \"\":\n",
    "        continue\n",
    "    result = chain.run(input_documents=docs, question=query)\n",
    "    print(f\"Answer: \" +result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90193b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
