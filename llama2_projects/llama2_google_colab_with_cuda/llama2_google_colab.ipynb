{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Instal all the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'CMAKE_ARGS' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting huggingface_hub\n",
      "  Using cached huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\lukas\\anaconda3\\lib\\site-packages (from huggingface_hub) (3.7.4.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lukas\\anaconda3\\lib\\site-packages (from huggingface_hub) (5.4.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\lukas\\anaconda3\\lib\\site-packages (from huggingface_hub) (0.9.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\lukas\\anaconda3\\lib\\site-packages (from huggingface_hub) (3.0.12)\n",
      "Requirement already satisfied: requests in c:\\users\\lukas\\anaconda3\\lib\\site-packages (from huggingface_hub) (2.25.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\lukas\\anaconda3\\lib\\site-packages (from huggingface_hub) (20.9)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\lukas\\anaconda3\\lib\\site-packages (from huggingface_hub) (4.59.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\lukas\\anaconda3\\lib\\site-packages (from packaging>=20.9->huggingface_hub) (2.4.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\lukas\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\lukas\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\lukas\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lukas\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (2020.12.5)\n",
      "Installing collected packages: huggingface-hub\n",
      "Successfully installed huggingface-hub-0.16.4\n"
     ]
    }
   ],
   "source": [
    "# GPU llama-cpp-python\n",
    "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir --verbose\n",
    "\n",
    "# For download the models\n",
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import all the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Download the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGML\"\n",
    "model_basename = \"llama-2-13b-chat.ggmlv3.q5_1.bin\" # the model is in bin format\n",
    "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "model_path = r\"D:/llama2_quantized_models/7B_chat/llama-2-7b-chat.ggmlv3.q8_0.bin\"\n",
    "\n",
    "# GPU\n",
    "lcpp_llm = None\n",
    "lcpp_llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_threads=2, # CPU cores\n",
    "    n_batch=512, # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "    n_gpu_layers=32 # Change this value based on your model and your GPU VRAM pool.\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the number of layers in GPU\n",
    "lcpp_llm.params.n_gpu_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create a Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write a linear regression in python and plot it using seaborn\"\n",
    "prompt_template=f'''SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\n",
    "\n",
    "USER: {prompt}\n",
    "\n",
    "ASSISTANT:\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Generating the Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    }
   ],
   "source": [
    "response=lcpp_llm(prompt=prompt_template, max_tokens=256, temperature=0.5, top_p=0.95,\n",
    "                  repeat_penalty=1.2, top_k=150,\n",
    "                  echo=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-a7cf69b8-754e-4dd9-89a6-e129c7a0f79c', 'object': 'text_completion', 'created': 1691079629, 'model': 'D:/llama2_quantized_models/7B_chat/llama-2-7b-chat.ggmlv3.q8_0.bin', 'choices': [{'text': 'SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\\n\\nUSER: Write a linear regression in python\\n\\nASSISTANT:\\nOf course! Here is an example of how to write a linear regression algorithm in Python using scikit-learn library:\\n```\\nimport pandas as pd\\nfrom sklearn import datasets\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.model_selection import train_test_split\\n\\n# Load the Boston Housing dataset (a classic regression problem)\\nboston = datasets.load_boston()\\nX = boston.data\\ny = boston.target\\n\\n# Split the data into training and testing sets\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\\n# Create a linear regression object and fit it to the training data\\nreg = LinearRegression()\\nreg.fit(X_train, y_train)\\n\\n# Make predictions on the testing set\\ny_pred = reg.predict(X_test)\\nprint(\"R-squared value:\", reg.score(X_test, y_test))\\n```\\nThis code uses the `LinearRegression` class from scikit-learn to fit a linear regression model to the training data', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 39, 'completion_tokens': 256, 'total_tokens': 295}}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\n",
      "\n",
      "USER: Write a linear regression in python and plot it using seaborn\n",
      "\n",
      "ASSISTANT:\n",
      "Of course! Here is an example of how to write a linear regression in Python using scikit-learn library and then plot the results using Seaborn:\n",
      "```\n",
      "# Import necessary libraries\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.metrics import mean_squared_error, r2_score\n",
      "import seaborn as sns\n",
      "\n",
      "# Generate some sample data\n",
      "np.random.seed(0)  # for reproducibility\n",
      "n_samples = 100\n",
      "X = np.random.rand(n_samples, 3)\n",
      "y = np.random.rand(n_samples) + 1\n",
      "\n",
      "# Create a linear regression model and fit the data\n",
      "model = LinearRegression()\n",
      "model.fit(X, y)\n",
      "\n",
      "# Calculate mean squared error and R-squared value\n",
      "mse = mean_squared_error(y, model.predict(X))\n",
      "r2 = r2_score(y, model.predict(X))\n",
      "print(\"Mean squared error:\", mse)\n",
      "print(\"R-squared value:\", r\n"
     ]
    }
   ],
   "source": [
    "print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
