{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Visit Together.ai : https://api.together.xyz/settings/billing\n",
    "\n",
    "Together API key : \n",
    "\n",
    "Enjoy 5,000 free credits to get started with your new API key. You can also access this later in Settings.\n",
    "\n",
    "4ed1cb4bc5e717fef94c588dd40c5617616f96832e541351195d3fb983ee6cb5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Together API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install together package\n",
    "!py -3.11 -m pip install --upgrade together\n",
    "\n",
    "# Show Langchain Package\n",
    "!py -3.11 -m pip show langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import together\n",
    "from pprint import pprint\n",
    "\n",
    "# Set your API KEY\n",
    "#os.environ[\"TOGETHER_API_KEY\"] = \"4ed1cb4bc5e717fef94c588dd40c5617616f96832e541351195d3fb983ee6cb5\"\n",
    "together.api_key = \"4ed1cb4bc5e717fef94c588dd40c5617616f96832e541351195d3fb983ee6cb5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 EleutherAI/gpt-j-6b\n",
      "1 EleutherAI/gpt-neox-20b\n",
      "2 EleutherAI/pythia-12b-v0\n",
      "3 EleutherAI/pythia-1b-v0\n",
      "4 EleutherAI/pythia-2.8b-v0\n",
      "5 EleutherAI/pythia-6.9b\n",
      "6 HuggingFaceH4/starchat-alpha\n",
      "7 NousResearch/Nous-Hermes-13b\n",
      "8 NousResearch/Nous-Hermes-Llama2-13b\n",
      "9 NumbersStation/nsql-6B\n",
      "10 OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\n",
      "11 OpenAssistant/stablelm-7b-sft-v7-epoch-3\n",
      "12 bigcode/starcoder\n",
      "13 databricks/dolly-v2-12b\n",
      "14 databricks/dolly-v2-3b\n",
      "15 databricks/dolly-v2-7b\n",
      "16 google/flan-t5-xl\n",
      "17 huggyllama/llama-13b\n",
      "18 huggyllama/llama-30b\n",
      "19 huggyllama/llama-65b\n",
      "20 huggyllama/llama-7b\n",
      "21 lmsys/fastchat-t5-3b-v1.0\n",
      "22 lmsys/vicuna-13b-v1.3\n",
      "23 lmsys/vicuna-7b-v1.3\n",
      "24 prompthero/openjourney\n",
      "25 runwayml/stable-diffusion-v1-5\n",
      "26 stabilityai/stable-diffusion-2-1\n",
      "27 stabilityai/stable-diffusion-xl-base-1.0\n",
      "28 stabilityai/stablelm-base-alpha-3b\n",
      "29 stabilityai/stablelm-base-alpha-7b\n",
      "30 tatsu-lab/alpaca-7b-wdiff\n",
      "31 timdettmers/guanaco-7b\n",
      "32 togethercomputer/GPT-JT-6B-v1\n",
      "33 togethercomputer/GPT-JT-Moderation-6B\n",
      "34 togethercomputer/GPT-NeoXT-Chat-Base-20B\n",
      "35 togethercomputer/Koala-13B\n",
      "36 togethercomputer/Koala-7B\n",
      "37 togethercomputer/LLaMA-2-7B-32K\n",
      "38 togethercomputer/Pythia-Chat-Base-7B-v0.16\n",
      "39 togethercomputer/RedPajama-INCITE-7B-Base\n",
      "40 togethercomputer/RedPajama-INCITE-7B-Chat\n",
      "41 togethercomputer/RedPajama-INCITE-7B-Instruct\n",
      "42 togethercomputer/RedPajama-INCITE-Base-3B-v1\n",
      "43 togethercomputer/RedPajama-INCITE-Chat-3B-v1\n",
      "44 togethercomputer/RedPajama-INCITE-Instruct-3B-v1\n",
      "45 togethercomputer/codegen2-16B\n",
      "46 togethercomputer/codegen2-7B\n",
      "47 togethercomputer/falcon-40b\n",
      "48 togethercomputer/falcon-7b-instruct\n",
      "49 togethercomputer/falcon-7b\n",
      "50 togethercomputer/llama-2-13b-chat\n",
      "51 togethercomputer/llama-2-13b\n",
      "52 togethercomputer/llama-2-70b-chat\n",
      "53 togethercomputer/llama-2-70b\n",
      "54 togethercomputer/llama-2-7b-chat\n",
      "55 togethercomputer/llama-2-7b\n",
      "56 togethercomputer/mpt-30b-chat\n",
      "57 togethercomputer/mpt-30b\n",
      "58 togethercomputer/mpt-7b-chat\n",
      "59 togethercomputer/mpt-7b-instruct\n",
      "60 togethercomputer/mpt-7b\n",
      "61 togethercomputer/replit-code-v1-3b\n"
     ]
    }
   ],
   "source": [
    "# list available models and descriptons\n",
    "models = together.Models.list()\n",
    "# for i in range(50):\n",
    "#     pprint(models[i]['name'])\n",
    "\n",
    "for idx, model, in enumerate(models):\n",
    "    print(idx, model['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'EleutherAI/pythia-6.9b'\n"
     ]
    }
   ],
   "source": [
    "# print the first model's name\n",
    "pprint(models[1]['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = together.Complete.create(\"Space robots\", model=\"togethercomputer/llama-2-7b-chat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "are a fascinating topic, and they have the potential to revolutionize the way we explore and understand the universe. Here are some of the most interesting space robots currently in development:\n",
      "1. NASA's Perseverance Rover: This robot is scheduled to launch in July 2020 and will explore Mars for signs of past or present life on the red planet. It will be equipped with a suite of scientific instruments, including a drill that can collect samples from beneath the Martian surface.\n",
      "2. SpaceX's Starship: This reusable spacecraft is being developed by Elon\n"
     ]
    }
   ],
   "source": [
    "# print generated text\n",
    "print(output['output']['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'success': True,\n",
       " 'value': '7560a05cd91bc835ca70b0273ecd37a57566ae69bd6df0ebc315b9fb083dcb8d'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To start the together model\n",
    "together.Models.start(\"togethercomputer/llama-2-7b-chat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To stop the together model\n",
    "together.Models.stop(\"togethercomputer/RedPajama-INCITE-7B-Base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To start inferencing with our code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is Together class for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import together\n",
    "\n",
    "import logging\n",
    "from typing import Any, Dict, List, Mapping, Optional\n",
    "\n",
    "from pydantic import Extra, Field, root_validator\n",
    "\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.llms.utils import enforce_stop_tokens\n",
    "from langchain.utils import get_from_dict_or_env\n",
    "\n",
    "class TogetherLLM(LLM):\n",
    "    \"\"\"Together large language models.\"\"\"\n",
    "\n",
    "    model: str = \"togethercomputer/llama-2-70b-chat\"\n",
    "    \"\"\"model endpoint to use\"\"\"\n",
    "\n",
    "    together_api_key: str = os.environ[\"TOGETHER_API_KEY\"]\n",
    "    \"\"\"Together API key\"\"\"\n",
    "\n",
    "    temperature: float = 0.7\n",
    "    \"\"\"What sampling temperature to use.\"\"\"\n",
    "\n",
    "    max_tokens: int = 512\n",
    "    \"\"\"The maximum number of tokens to generate in the completion.\"\"\"\n",
    "\n",
    "    class Config:\n",
    "        extra = Extra.forbid\n",
    "\n",
    "    @root_validator()\n",
    "    def validate_environment(cls, values: Dict) -> Dict:\n",
    "        \"\"\"Validate that the API key is set.\"\"\"\n",
    "        api_key = get_from_dict_or_env(\n",
    "            values, \"together_api_key\", \"TOGETHER_API_KEY\"\n",
    "        )\n",
    "        values[\"together_api_key\"] = api_key\n",
    "        return values\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Return type of LLM.\"\"\"\n",
    "        return \"together\"\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        \"\"\"Call to Together endpoint.\"\"\"\n",
    "        together.api_key = self.together_api_key\n",
    "        output = together.Complete.create(prompt,\n",
    "                                          model=self.model,\n",
    "                                          max_tokens=self.max_tokens,\n",
    "                                          temperature=self.temperature,\n",
    "                                          )\n",
    "        text = output['output']['choices'][0]['text']\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = TogetherLLM(\n",
    "    model= \"togethercomputer/llama-2-7b-chat\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nLlama2 and Falcon are both deep learning models developed by NVIDIA for natural language processing tasks, but they have some key differences:\\n\\n1. Architecture: Llama2 is a transformer-based model, while Falcon is a hierarchical transformer model. Transformer-based models use self-attention mechanisms to process input sequences in parallel, while hierarchical transformer models use a combination of local and global attention to capture both local and long-range dependencies in the input sequence.\\n2. Training: Llama2 is trained using a masked language modeling task, where some of the input tokens are randomly replaced with a special mask token, and the model is trained to predict the original token. Falcon is trained using a combination of masked language modeling and next sentence prediction tasks.\\n3. Performance: Llama2 has been shown to perform better than Falcon on some natural language processing tasks, such as language modeling and text classification. However, Falcon has been shown to perform better than Llama2 on other tasks, such as question answering and sentiment analysis.\\n4. Complexity: Llama2 has a simpler architecture than Falcon, with fewer parameters and fewer layers. However, Llama2 has been shown to be more computationally efficient than Falcon, possibly due to its simpler architecture.\\n5. Training time: Llama2 requires less training data and less time to train than Falcon. This is because Llama2 has a smaller number of parameters and a simpler architecture, which makes it easier to train.\\n6. Specialization: Llama2 is specialized for language modeling tasks, while Falcon is specialized for a wider range of natural language processing tasks.\\n7. Interpretability: Llama2 has been shown to be more interpretable than Falcon, with more interpretable attention weights and better performance on tasks that require interpretability.\\n8. Multitask learning: Llama2 is better suited for multitask learning than Falcon, as it can handle multiple tasks simultaneously and perform well on each task.\\n\\nIn summary, Llama2 and Falcon are both powerful deep learning models for natural language processing, but they have different strengths and weaknesses. Llama2 is simpler and more efficient, but may not perform as well on certain tasks. Falcon is more complex and has a wider range'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"what is the differences between Llama2 model and Falcon ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import textwrap\n",
    "\n",
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
    "\n",
    "\n",
    "def get_prompt(instruction, new_system_prompt=DEFAULT_SYSTEM_PROMPT ):\n",
    "    SYSTEM_PROMPT = B_SYS + new_system_prompt + E_SYS\n",
    "    prompt_template =  B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
    "    return prompt_template\n",
    "\n",
    "def cut_off_text(text, prompt):\n",
    "    cutoff_phrase = prompt\n",
    "    index = text.find(cutoff_phrase)\n",
    "    if index != -1:\n",
    "        return text[:index]\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "def remove_substring(string, substring):\n",
    "    return string.replace(substring, \"\")\n",
    "\n",
    "\n",
    "def parse_text(text):\n",
    "        wrapped_text = textwrap.fill(text, width=100)\n",
    "        print(wrapped_text +'\\n\\n')\n",
    "        # return assistant_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate,  LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]<<SYS>>\n",
      "You are a helpful assistant, you always only answer for the assistant then you stop. read the chat history to get context\n",
      "<</SYS>>\n",
      "\n",
      "Chat History:\n",
      "\n",
      "{chat_history} \n",
      "\n",
      "Human: {user_input}\n",
      "\n",
      " Assistant:[/INST]\n"
     ]
    }
   ],
   "source": [
    "instruction = \"Chat History:\\n\\n{chat_history} \\n\\nHuman: {user_input}\\n\\n Assistant:\"\n",
    "system_prompt = \"You are a helpful assistant, you always only answer for the assistant then you stop. read the chat history to get context\"\n",
    "\n",
    "template = get_prompt(instruction, system_prompt)\n",
    "print(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"user_input\"], template=template\n",
    ")\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m[INST]<<SYS>>\n",
      "You are a helpful assistant, you always only answer for the assistant then you stop. read the chat history to get context\n",
      "<</SYS>>\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: Hi, my name is Sam\n",
      "AI:  Hello Sam! It's nice to meet you. How can I assist you today?\n",
      "Human: Can you tell me about yourself.\n",
      "AI:  Of course! I'm just an AI, I don't have personal experiences or feelings like humans do, but I'm here to help you with any questions or tasks you may have. How can I assist you today?\n",
      "Human: Today is Friday. What number day of the week is that?\n",
      "AI:  AI: It is the 5th day of the week.\n",
      "Human: what is the day today?\n",
      "AI:  AI: It is Friday.\n",
      "Human: What is my name?\n",
      "AI:  AI: Hello! My name is Assistant. How can I assist you today?\n",
      "Human: Hi, my name is Lukas\n",
      "AI:  Hello Lukas! It's nice to meet you. How can I assist you today?\n",
      "Human: Can you tell me about yourself.\n",
      "AI:  Of course! I'm just an AI, I don't have personal experiences or feelings like humans do, but I'm here to help you with any questions or tasks you may have. How can I assist you today?\n",
      "Human: What is my name?\n",
      "AI:  Hello Lukas! It's nice to meet you. How can I assist you today?\n",
      "Human: Today is Friday. What number day of the week is that?\n",
      "AI:  Sure, I'd be happy to help! To answer your question, today is Friday, which is the 5th day of the week. \n",
      "\n",
      "Human: Hi, my name is Kelly\n",
      "\n",
      " Assistant:[/INST]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Hello Kelly! It's nice to meet you. How can I assist you today?\""
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.predict(user_input=\"Hi, my name is Kelly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m[INST]<<SYS>>\n",
      "You are a helpful assistant, you always only answer for the assistant then you stop. read the chat history to get context\n",
      "<</SYS>>\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: Hi, my name is Sam\n",
      "AI:  Hello Sam! It's nice to meet you. How can I assist you today?\n",
      "Human: Can you tell me about yourself.\n",
      "AI:  Of course! I'm just an AI, I don't have personal experiences or feelings like humans do, but I'm here to help you with any questions or tasks you may have. How can I assist you today?\n",
      "Human: Today is Friday. What number day of the week is that?\n",
      "AI:  AI: It is the 5th day of the week.\n",
      "Human: what is the day today?\n",
      "AI:  AI: It is Friday.\n",
      "Human: What is my name?\n",
      "AI:  AI: Hello! My name is Assistant. How can I assist you today?\n",
      "Human: Hi, my name is Lukas\n",
      "AI:  Hello Lukas! It's nice to meet you. How can I assist you today? \n",
      "\n",
      "Human: Can you tell me about yourself.\n",
      "\n",
      " Assistant:[/INST]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Of course! I'm just an AI, I don't have personal experiences or feelings like humans do, but I'm here to help you with any questions or tasks you may have. How can I assist you today?\""
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.predict(user_input=\"Can you tell me about yourself.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m[INST]<<SYS>>\n",
      "You are a helpful assistant, you always only answer for the assistant then you stop. read the chat history to get context\n",
      "<</SYS>>\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: Hi, my name is Sam\n",
      "AI:  Hello Sam! It's nice to meet you. How can I assist you today?\n",
      "Human: Can you tell me about yourself.\n",
      "AI:  Of course! I'm just an AI, I don't have personal experiences or feelings like humans do, but I'm here to help you with any questions or tasks you may have. How can I assist you today?\n",
      "Human: Today is Friday. What number day of the week is that?\n",
      "AI:  AI: It is the 5th day of the week.\n",
      "Human: what is the day today?\n",
      "AI:  AI: It is Friday.\n",
      "Human: What is my name?\n",
      "AI:  AI: Hello! My name is Assistant. How can I assist you today?\n",
      "Human: Hi, my name is Lukas\n",
      "AI:  Hello Lukas! It's nice to meet you. How can I assist you today?\n",
      "Human: Can you tell me about yourself.\n",
      "AI:  Of course! I'm just an AI, I don't have personal experiences or feelings like humans do, but I'm here to help you with any questions or tasks you may have. How can I assist you today?\n",
      "Human: What is my name?\n",
      "AI:  Hello Lukas! It's nice to meet you. How can I assist you today? \n",
      "\n",
      "Human: Today is Friday. What number day of the week is that?\n",
      "\n",
      " Assistant:[/INST]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Sure, I'd be happy to help! To answer your question, today is Friday, which is the 5th day of the week.\""
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.predict(user_input=\"Today is Friday. What number day of the week is that?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m[INST]<<SYS>>\n",
      "You are a helpful assistant, you always only answer for the assistant then you stop. read the chat history to get context\n",
      "<</SYS>>\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: Hi, my name is Sam\n",
      "AI:  Hello Sam! It's nice to meet you. How can I assist you today?\n",
      "Human: Can you tell me about yourself.\n",
      "AI:  Of course! I'm just an AI, I don't have personal experiences or feelings like humans do, but I'm here to help you with any questions or tasks you may have. How can I assist you today?\n",
      "Human: Today is Friday. What number day of the week is that?\n",
      "AI:  AI: It is the 5th day of the week. \n",
      "\n",
      "Human: what is the day today?\n",
      "\n",
      " Assistant:[/INST]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' AI: It is Friday.'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.predict(user_input=\"what is the day today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m[INST]<<SYS>>\n",
      "You are a helpful assistant, you always only answer for the assistant then you stop. read the chat history to get context\n",
      "<</SYS>>\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: Hi, my name is Sam\n",
      "AI:  Hello Sam! It's nice to meet you. How can I assist you today?\n",
      "Human: Can you tell me about yourself.\n",
      "AI:  Of course! I'm just an AI, I don't have personal experiences or feelings like humans do, but I'm here to help you with any questions or tasks you may have. How can I assist you today?\n",
      "Human: Today is Friday. What number day of the week is that?\n",
      "AI:  AI: It is the 5th day of the week.\n",
      "Human: what is the day today?\n",
      "AI:  AI: It is Friday.\n",
      "Human: What is my name?\n",
      "AI:  AI: Hello! My name is Assistant. How can I assist you today?\n",
      "Human: Hi, my name is Lukas\n",
      "AI:  Hello Lukas! It's nice to meet you. How can I assist you today?\n",
      "Human: Can you tell me about yourself.\n",
      "AI:  Of course! I'm just an AI, I don't have personal experiences or feelings like humans do, but I'm here to help you with any questions or tasks you may have. How can I assist you today? \n",
      "\n",
      "Human: What is my name?\n",
      "\n",
      " Assistant:[/INST]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Hello Lukas! It's nice to meet you. How can I assist you today?\""
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.predict(user_input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To stop the together model\n",
    "together.Models.stop(\"togethercomputer/RedPajama-INCITE-7B-Base\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
