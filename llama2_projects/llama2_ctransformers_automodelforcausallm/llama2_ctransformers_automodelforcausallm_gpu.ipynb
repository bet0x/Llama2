{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started\n",
    "\n",
    "Install following libraries\n",
    "```\n",
    "For CPU\n",
    "!pip install ctransformers -q \n",
    "\n",
    "For GPU\n",
    "!pip install ctransformers[cuda]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctransformers import AutoModelForCausalLM\n",
    "model_path = r\"D:/llama2_quantized_models/7B_q5/llama-2-7b-chat.ggmlv3.q5_K_M.bin\"\n",
    "\n",
    "model_name = \"llama-2-7b-chat.ggmlv3.q5_K_M.bin\"\n",
    "\n",
    "#Run with CPU\n",
    "#llm = AutoModelForCausalLM.from_pretrained('TheBloke/Llama-2-7B-Chat-GGML', model_file=model)\n",
    "\n",
    "# Run with GPU\n",
    "#llm = AutoModelForCausalLM.from_pretrained('TheBloke/Llama-2-7B-Chat-GGML', model_file=model_path, gpu_layers=35)\n",
    "\n",
    "# Run and load model locally with GPU\n",
    "llm = AutoModelForCausalLM.from_pretrained(model_path_or_repo_id=model_path, model_file=model_name, gpu_layers=35, model_type='llama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in llm(\"Write me a melayu song about love to my girlfriend?\", stream=True):\n",
    "    print(word, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain import LLMChain\n",
    "from langchain.llms import CTransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "\n",
    "DEFAULT_SYSTEM_PROMPT=\"\"\"\\\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
    "\n",
    "instruction = \"write me skillset list set need for Data Scientis engineer \\n\\n {text}\"\n",
    "SYSTEM_PROMPT = B_SYS + DEFAULT_SYSTEM_PROMPT + E_SYS\n",
    "template = B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
    "\n",
    "template = \"\"\"[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "\n",
    "<</SYS>>\n",
    "\n",
    "{question}\n",
    "\n",
    "[/INST]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] <<SYS>>\n",
      "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
      "\n",
      "<</SYS>>\n",
      "\n",
      "{question}\n",
      "\n",
      "[/INST]\n"
     ]
    }
   ],
   "source": [
    "print(template)\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL_PATH = r\"D:/llama2_quantized_models/7B_chat/llama-2-7b-chat.ggmlv3.q8_0.bin\"\n",
    "MODEL_PATH = r\"D:/llama2_quantized_models/7B_chat/llama-2-7b-chat.ggmlv3.q5_K_M.bin\"\n",
    "\n",
    "llm = CTransformers(model=MODEL_PATH,\n",
    "                    model_type='llama',\n",
    "                    config={'max_new_tokens': 128,\n",
    "                            'temperature': 0.01,\n",
    "                            'gpu_layers': 35,\n",
    "                            'stream' : True,\n",
    "                            },\n",
    "                    \n",
    "                   )\n",
    "LLM_Chain=LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = (LLM_Chain.run(\"Write me a lyrics of english song about heart break\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Of course, I'd be happy to help you with that! Here's a possible English song lyric about heartbreak:\n",
      "\n",
      "Verse 1:\n",
      "My heart is breaking, it's tearing me apart\n",
      "The pain of losing you, it's ripping me to shreds\n",
      "I thought our love would last, but now I'm left with this scar\n",
      "A reminder of the love we had, and how far we are\n",
      "\n",
      "Chorus:\n",
      "Oh, heartbreak, why must you come my way?\n",
      "Taking away the one thing that made my life worth stay"
     ]
    }
   ],
   "source": [
    "for word in result:\n",
    "    print(word, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLAMA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
